{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aggregate-storm",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-mortgage",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "# %load_ext nb_black # for jupyter nootebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-avatar",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likely-mainland",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-might",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scheduled-roads",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from matplotlib.cm import tab10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"seaborn-notebook\")\n",
    "\n",
    "SMALL_SIZE = 12\n",
    "MEDIUM_SIZE = 15\n",
    "BIGGER_SIZE = 18\n",
    "\n",
    "plt.rc(\"font\", size=SMALL_SIZE)\n",
    "plt.rc(\"axes\", titlesize=BIGGER_SIZE)\n",
    "plt.rc(\"axes\", labelsize=MEDIUM_SIZE)\n",
    "plt.rc(\"xtick\", labelsize=SMALL_SIZE)\n",
    "plt.rc(\"ytick\", labelsize=SMALL_SIZE)\n",
    "plt.rc(\"legend\", fontsize=SMALL_SIZE)\n",
    "plt.rc(\"figure\", titlesize=BIGGER_SIZE)\n",
    "\n",
    "r = matplotlib.patches.Rectangle(\n",
    "    (0, 0), 1, 1, fill=False, edgecolor=\"none\", visible=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-multiple",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets():\n",
    "    ac_frame = pd.read_csv(\"../data/agatha_christie.csv\")\n",
    "\n",
    "    X_ac = ac_frame[\"text\"]\n",
    "    y_ac = ac_frame[\"book\"]\n",
    "\n",
    "    np_frame = pd.read_csv(\"../data/newspaper_articles.csv\")\n",
    "\n",
    "    X_np = np_frame[\"STORY\"]\n",
    "    y_np = np_frame[\"SECTION\"].map(\n",
    "        {0: \"Politics\", 1: \"Technology\", 2: \"Entertainment\", 3: \"Business\"}\n",
    "    )\n",
    "\n",
    "    ja_frame = pd.read_csv(\"../data/jane_austen.csv\")\n",
    "\n",
    "    X_ja = ja_frame[\"x_text\"]\n",
    "    y_ja = ja_frame[\"y_book\"].apply(\n",
    "        lambda x: \" \".join(y.capitalize() for y in x.split(\"_\"))\n",
    "    )\n",
    "\n",
    "    sh_frame = pd.read_csv(\"../data/sherlock_holmes.csv\")\n",
    "\n",
    "    X_sh = sh_frame[\"rawtext\"]\n",
    "    y_sh = sh_frame[\"label\"].map(\n",
    "        {\n",
    "            0: \"The Valley of Fear\",\n",
    "            1: \"The Memoirs of Sherlock Holmes\",\n",
    "            2: \"The Return of Sherlock Holmes\",\n",
    "            3: \"Adventures of Sherlock Holmes\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return [X_ac, X_sh, X_ja, X_np], [y_ac, y_sh, y_ja, y_np]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polyphonic-pioneer",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"Agatha Christie\", \"Sherlock Holmes\", \"Jane Austen\", \"Newspapers\"]\n",
    "X_datasets, y_datasets = load_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-doubt",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "table = str.maketrans(\"\", \"\", string.punctuation + \"——\")\n",
    "\n",
    "\n",
    "def remove_stop_words_and_tokenize(text: str) -> str:\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "\n",
    "    words = [stemmer.stem(w) for w in words]\n",
    "\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-cheese",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_prep = []\n",
    "\n",
    "for name, X in tqdm(zip(names, X_datasets), total=4):\n",
    "    X_prep.append(\n",
    "        [remove_stop_words_and_tokenize(x) for x in tqdm(X.values, desc=name)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boxed-champagne",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "balanced-centre",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-moderator",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from umap import UMAP\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-evanescence",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-dryer",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [\"agatha_christie\", \"sherlock_holmes\", \"jane_austen\", \"newspaper_articles\"]\n",
    "embeddings = [\"tfidf\", \"average_fasttext\", \"avg_glove\", \"distiluse\", \"roberta\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-confusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_emb = []\n",
    "for embedding in embeddings[1:]:\n",
    "    X_emb = []\n",
    "    for file_name in tqdm(file_names, desc=embedding):\n",
    "        X_emb.append(pd.read_csv(f\"../data/{file_name}-{embedding}.csv\", index_col=0))\n",
    "    X_all_emb.append(X_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-being",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs1 = re.compile(r\"[\\ \\(].*\")\n",
    "rs2 = re.compile(r\"[<].*\\.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-employment",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(shuffle=True, random_state=42)\n",
    "\n",
    "for embedding, X_emb in tqdm(zip(embeddings[1:], X_all_emb), total=len(embeddings[1:])):\n",
    "\n",
    "    for name, file_name, X, y_l in tqdm(\n",
    "        zip(names, file_names, X_emb, y_datasets), desc=embedding, total=len(names)\n",
    "    ):\n",
    "        y = LabelEncoder().fit_transform(y_l)\n",
    "\n",
    "        results = []\n",
    "\n",
    "        for i, indexes in tqdm(enumerate(kfold.split(X.values, y)), total=5, desc=name):\n",
    "            train_index, test_index = indexes\n",
    "\n",
    "            for prep in tqdm(\n",
    "                (\n",
    "                    None,\n",
    "                    PCA(n_components=50, random_state=42),\n",
    "                ),\n",
    "                desc=f\"Split: {i}\",\n",
    "            ):\n",
    "                \n",
    "                prep_start_time = time()\n",
    "                \n",
    "                X_train = X.values[train_index]\n",
    "                y_train = y[train_index]\n",
    "\n",
    "                X_test = X.values[test_index]\n",
    "                y_test = y[test_index]\n",
    "                \n",
    "                if prep is not None:\n",
    "                    X_train = prep.fit_transform(X_train, y_train)\n",
    "                    X_test = prep.transform(X_test)\n",
    "                \n",
    "                prep_end_time = time()\n",
    "\n",
    "                for clf in tqdm(\n",
    "                    (\n",
    "                        KNeighborsClassifier(metric=\"cosine\"),\n",
    "                        LinearSVC(),\n",
    "                        MLPClassifier(random_state=42),                        \n",
    "                        CatBoostClassifier(thread_count=16, random_state=42, verbose=0),\n",
    "                        LGBMClassifier(n_jobs=6, random_state=42, silent=True),\n",
    "                        XGBClassifier(n_jobs=6, random_state=42, verbosity=0),\n",
    "                    ),\n",
    "                    desc=str(prep),\n",
    "                ):\n",
    "                    clf_start_time = time()\n",
    "                    \n",
    "                    clf.fit(X_train, y_train)\n",
    "\n",
    "                    predicted = clf.predict(X_test)\n",
    "\n",
    "                    acc = accuracy_score(y_true=y_test, y_pred=predicted)\n",
    "                    \n",
    "                    clf_end_time = time()\n",
    "                    \n",
    "                    results.append(\n",
    "                        {\n",
    "                            \"embedding\": embedding,\n",
    "                            \"name\": name,\n",
    "                            \"split\": i,\n",
    "                            \"preprocessing\": str(prep),\n",
    "                            \"classifier\": rs1.sub(\"\", rs2.sub(\"\", str(clf))).strip(),\n",
    "                            \"accuracy\": acc,\n",
    "                            \"prep_time\": prep_end_time - prep_start_time,\n",
    "                            \"clf_time\": clf_end_time - clf_start_time,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        pd.DataFrame(results).round(4).to_csv(f\"../results/{file_name}-{embedding}-results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-atlantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Any, Optional\n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-rebel",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = KeyedVectors.load_word2vec_format(\n",
    "    \"wiki-news-300d-1M.vec\", binary=False, limit=200_000\n",
    ")\n",
    "\n",
    "\n",
    "class FastTextIDFTransformer(TransformerMixin):\n",
    "    __splitter = re.compile(r\"[\\W_]\")\n",
    "    __table = str.maketrans(\"\", \"\", string.punctuation)\n",
    "\n",
    "    def __init__(self, func: str = \"mean\", stop_words: Iterable[str] = []) -> None:\n",
    "        self.func = func\n",
    "        self.stop_words = stop_words\n",
    "\n",
    "    def fit(self, X: Any, y: Any = None) -> \"FastTextTransformer\":\n",
    "\n",
    "        tfidf = TfidfVectorizer(stop_words=self.stop_words)\n",
    "        tfidf.fit(X)\n",
    "\n",
    "        self.max_idf = max(tfidf.idf_)\n",
    "\n",
    "        self.idfs = {\n",
    "            word: idf for word, idf in zip(tfidf.get_feature_names(), tfidf.idf_)\n",
    "        }\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _word_vec(self, word: str) -> Optional[np.ndarray]:\n",
    "\n",
    "        if word in w2v:\n",
    "            if word in self.idfs:\n",
    "                return w2v[word] * self.idfs[word]\n",
    "            else:\n",
    "                return w2v[word] * self.max_idf\n",
    "\n",
    "        return None\n",
    "\n",
    "    def transform(self, X: Any, y: Any = None) -> np.ndarray:\n",
    "        results = []\n",
    "\n",
    "        for x in X:\n",
    "            vec = []\n",
    "\n",
    "            tokens = word_tokenize(x)\n",
    "\n",
    "            for token in tokens:\n",
    "                w = self._word_vec(token)\n",
    "\n",
    "                if w is not None:\n",
    "                    vec.append(w)\n",
    "                    continue\n",
    "                else:\n",
    "                    for sub in self.__splitter.split(token):\n",
    "                        w = self._word_vec(token)\n",
    "                        if w is not None:\n",
    "                            vec.append(w)\n",
    "                            continue\n",
    "\n",
    "            if self.func == \"mean\":\n",
    "                results.append(np.mean(vec, axis=0))\n",
    "            else:\n",
    "                results.append(np.sum(vec, axis=0))\n",
    "\n",
    "        return np.vstack(results)\n",
    "\n",
    "    def fit_transform(self, X: Any, y: Any = None) -> np.ndarray:\n",
    "        self.fit(X)\n",
    "\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flexible-circulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(shuffle=True, random_state=42)\n",
    "\n",
    "signi_stop_words = {}\n",
    "\n",
    "for embedding in tqdm([\"tfidf\", \"tfidf_wosigni\"]):\n",
    "\n",
    "    for name, file_name, X, y_l in tqdm(\n",
    "        zip(names, file_names, X_prep, y_datasets), desc=embedding, total=len(names)\n",
    "    ):\n",
    "        y = LabelEncoder().fit_transform(y_l)\n",
    "\n",
    "        results = []\n",
    "        \n",
    "        if file_name.startswith(\"news\") and embedding.endswith(\"gni\"):\n",
    "            continue\n",
    "        \n",
    "        X = np.array(X)\n",
    "\n",
    "        for i, indexes in tqdm(enumerate(kfold.split(X, y)), total=5, desc=name):\n",
    "            train_index, test_index = indexes\n",
    "\n",
    "            for prep in tqdm(\n",
    "                (\n",
    "                    None,\n",
    "                    PCA(n_components=50, random_state=42),\n",
    "                    TruncatedSVD(n_components=50, random_state=42),\n",
    "                ),\n",
    "                desc=f\"Split: {i}\",\n",
    "            ):\n",
    "                        \n",
    "                X_train = X[train_index]\n",
    "                y_train = y[train_index]\n",
    "\n",
    "                X_test = X[test_index]\n",
    "                y_test = y[test_index]\n",
    "\n",
    "                tfidf = TfidfVectorizer()\n",
    "\n",
    "                if embedding == \"tfidf\":\n",
    "                    tfidf = TfidfVectorizer()\n",
    "                else:\n",
    "                    tfidf = TfidfVectorizer(stop_words=signi_stop_words[name])\n",
    "\n",
    "                X_train = tfidf.fit_transform(X_train)\n",
    "                X_test = tfidf.transform(X_test)\n",
    "\n",
    "                if embedding == \"tfidf\":\n",
    "                    tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "                    tree.fit(X_train, y_train)\n",
    "\n",
    "                    features = np.array(tfidf.get_feature_names())\n",
    "\n",
    "                    ns_stop = features[tree.feature_importances_ > 0]\n",
    "\n",
    "                    signi_stop_words.setdefault(name, set())\n",
    "\n",
    "                    for ns in ns_stop:\n",
    "                        signi_stop_words[name].add(ns)\n",
    "\n",
    "                prep_start_time = time()\n",
    "\n",
    "                if prep is not None:\n",
    "                    X_train = prep.fit_transform(X_train, y_train)\n",
    "                    X_test = prep.transform(X_test)\n",
    "\n",
    "                prep_end_time = time()\n",
    "\n",
    "                for clf in tqdm(\n",
    "                    (\n",
    "                        KNeighborsClassifier(metric=\"cosine\"),\n",
    "                        LinearSVC(),\n",
    "                        MLPClassifier(random_state=42),\n",
    "                        MultinomialNB(),\n",
    "                        ComplementNB(),\n",
    "                        LGBMClassifier(n_jobs=6, random_state=42, silent=True),\n",
    "                        XGBClassifier(n_jobs=6, random_state=42, verbosity=0),\n",
    "                    ),\n",
    "                    desc=str(prep),\n",
    "                ):\n",
    "\n",
    "                    clf_start_time = time()\n",
    "\n",
    "                    clf.fit(X_train, y_train)\n",
    "\n",
    "                    predicted = clf.predict(X_test)\n",
    "\n",
    "                    acc = accuracy_score(y_true=y_test, y_pred=predicted)\n",
    "\n",
    "                    clf_end_time = time()\n",
    "\n",
    "                    results.append(\n",
    "                        {\n",
    "                            \"embedding\": embedding,\n",
    "                            \"name\": name,\n",
    "                            \"split\": i,\n",
    "                            \"preprocessing\": str(prep),\n",
    "                            \"classifier\": rs1.sub(\"\", rs2.sub(\"\", str(clf))).strip(),\n",
    "                            \"accuracy\": acc,\n",
    "                            \"prep_time\": prep_end_time - prep_start_time,\n",
    "                            \"clf_time\": clf_end_time - clf_start_time,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        pd.DataFrame(results).round(4).to_csv(\n",
    "            f\"../results/{file_name}-{embedding}-results.csv\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-creator",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(shuffle=True, random_state=42)\n",
    "\n",
    "for embedding in tqdm([\"fasttextidf\", \"fasttextidf_wosigni\"]):\n",
    "\n",
    "    for name, file_name, X, y_l in tqdm(\n",
    "        zip(names, file_names, X_prep, y_datasets), desc=embedding, total=len(names)\n",
    "    ):\n",
    "        y = LabelEncoder().fit_transform(y_l)\n",
    "\n",
    "        results = []\n",
    "\n",
    "        if file_name.startswith(\"news\") and embedding.endswith(\"gni\"):\n",
    "            continue\n",
    "\n",
    "        X = np.array(X)\n",
    "\n",
    "        for i, indexes in tqdm(enumerate(kfold.split(X, y)), total=5, desc=name):\n",
    "            train_index, test_index = indexes\n",
    "\n",
    "            for prep in tqdm(\n",
    "                (\n",
    "                    None,\n",
    "                    PCA(n_components=50, random_state=42),\n",
    "                ),\n",
    "                desc=f\"Split: {i}\",\n",
    "            ):\n",
    "\n",
    "                X_train = X[train_index]\n",
    "                y_train = y[train_index]\n",
    "\n",
    "                X_test = X[test_index]\n",
    "                y_test = y[test_index]\n",
    "\n",
    "                if embedding == \"fasttextidf\":\n",
    "                    tfidf = FastTextIDFTransformer()\n",
    "                else:\n",
    "                    tfidf = FastTextIDFTransformer(stop_words=signi_stop_words[name])\n",
    "\n",
    "                X_train = tfidf.fit_transform(X_train)\n",
    "                X_test = tfidf.transform(X_test)\n",
    "\n",
    "                prep_start_time = time()\n",
    "\n",
    "                if prep is not None:\n",
    "                    X_train = prep.fit_transform(X_train, y_train)\n",
    "                    X_test = prep.transform(X_test)\n",
    "\n",
    "                prep_end_time = time()\n",
    "\n",
    "                for clf in tqdm(\n",
    "                    (\n",
    "                        KNeighborsClassifier(metric=\"cosine\"),\n",
    "                        LinearSVC(),\n",
    "                        MLPClassifier(random_state=42),\n",
    "                        CatBoostClassifier(thread_count=16, random_state=42, verbose=0),\n",
    "                        LGBMClassifier(n_jobs=6, random_state=42, silent=True),\n",
    "                        XGBClassifier(n_jobs=6, random_state=42, verbosity=0),\n",
    "                    ),\n",
    "                    desc=str(prep),\n",
    "                ):\n",
    "\n",
    "                    clf_start_time = time()\n",
    "\n",
    "                    clf.fit(X_train, y_train)\n",
    "\n",
    "                    predicted = clf.predict(X_test)\n",
    "\n",
    "                    acc = accuracy_score(y_true=y_test, y_pred=predicted)\n",
    "\n",
    "                    clf_end_time = time()\n",
    "\n",
    "                    results.append(\n",
    "                        {\n",
    "                            \"embedding\": embedding,\n",
    "                            \"name\": name,\n",
    "                            \"split\": i,\n",
    "                            \"preprocessing\": str(prep),\n",
    "                            \"classifier\": rs1.sub(\"\", rs2.sub(\"\", str(clf))).strip(),\n",
    "                            \"accuracy\": acc,\n",
    "                            \"prep_time\": prep_end_time - prep_start_time,\n",
    "                            \"clf_time\": clf_end_time - clf_start_time,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        pd.DataFrame(results).round(4).to_csv(\n",
    "            f\"../results/{file_name}-{embedding}-results.csv\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powered-devon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "future-marking",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abroad-provincial",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PA164 ML and NLP",
   "language": "python",
   "name": "pa164"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
