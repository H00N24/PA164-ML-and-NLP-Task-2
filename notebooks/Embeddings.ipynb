{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "suspended-vinyl",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-summer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from typing import Any, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-costa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_frame = pd.read_csv(\"../data/agatha_christie.csv\")\n",
    "\n",
    "X_ac = ac_frame[\"text\"]\n",
    "y_ac = ac_frame[\"book\"]\n",
    "\n",
    "np_frame = pd.read_csv(\"../data/newspaper_articles.csv\")\n",
    "\n",
    "X_np = np_frame[\"STORY\"]\n",
    "y_np = np_frame[\"SECTION\"].map(\n",
    "    {0: \"Politics\", 1: \"Technology\", 2: \"Entertainment\", 3: \"Business\"}\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "ja_frame = pd.read_csv(\"../data/jane_austen.csv\")\n",
    "\n",
    "X_ja = ja_frame[\"x_text\"]\n",
    "y_ja = ja_frame[\"y_book\"]\n",
    "\n",
    "sh_frame = pd.read_csv(\"../data/sherlock_holmes.csv\")\n",
    "\n",
    "\n",
    "\n",
    "X_sh = sh_frame[\"rawtext\"]\n",
    "y_sh = sh_frame[\"label\"].map(\n",
    "    {\n",
    "        0: \"The Valley of Fear\",\n",
    "        1: \"The Memoirs of Sherlock Holmes\",\n",
    "        2: \"The Return of Sherlock Holmes\",\n",
    "        3: \"Adventures of Sherlock Holmes\",\n",
    "    }\n",
    ")\n",
    "\n",
    "names = [\"Agatha Christie\", \"Sherlock Holmes\", \"Jane Austen\", \"Newspapers\"]\n",
    "X_datasets = [X_ac, X_sh, X_ja, X_np]\n",
    "y_datasets = [y_ac, y_sh, y_ja, y_np]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-library",
   "metadata": {},
   "source": [
    "## Avg FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-swaziland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip && unzip wiki-news-300d-1M.vec.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-liberty",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "class FastTextTransformer(TransformerMixin):\n",
    "    __splitter = re.compile(r\"[\\W_]\")\n",
    "    __table = str.maketrans(\"\", \"\", string.punctuation)\n",
    "\n",
    "    def __init__(self, func: str = \"mean\") -> None:\n",
    "        self.func = func\n",
    "\n",
    "    def fit(self, X: Any, y: Any = None) -> \"FastTextTransformer\":\n",
    "        return self\n",
    "\n",
    "    def _word_vec(self, word: str) -> Optional[np.ndarray]:\n",
    "\n",
    "        for w in (\n",
    "            word,\n",
    "            word.lower(),\n",
    "            word.translate(self.__table),\n",
    "            word.translate(self.__table).lower(),\n",
    "            stemmer.stem(word.translate(self.__table).lower()),\n",
    "        ):\n",
    "            if w in stop_words:\n",
    "                return None\n",
    "\n",
    "            if w in w2v:\n",
    "                return w2v[w]\n",
    "\n",
    "        return None\n",
    "\n",
    "    def transform(self, X: Any, y: Any = None) -> np.ndarray:\n",
    "        results = []\n",
    "\n",
    "        for x in X:\n",
    "            vec = []\n",
    "\n",
    "            tokens = word_tokenize(x)\n",
    "\n",
    "            for token in tokens:\n",
    "                w = self._word_vec(token)\n",
    "\n",
    "                if w is not None:\n",
    "                    vec.append(w)\n",
    "                    continue\n",
    "                else:\n",
    "                    for sub in self.__splitter.split(token):\n",
    "                        w = self._word_vec(token)\n",
    "                        if w is not None:\n",
    "                            vec.append(w)\n",
    "                            continue\n",
    "\n",
    "            if self.func == \"mean\":\n",
    "                results.append(np.mean(vec, axis=0))\n",
    "            else:\n",
    "                results.append(np.sum(vec, axis=0))\n",
    "\n",
    "        return np.vstack(results)\n",
    "\n",
    "        def transform(self, X: Any, y: Any = None) -> np.ndarray:\n",
    "            return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-jonathan",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = FastTextTransformer()\n",
    "\n",
    "X_fasttext = []\n",
    "\n",
    "for name, X in tqdm(zip(names, X_datasets), total=4):\n",
    "    X_fasttext.append(fs.fit_transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-amazon",
   "metadata": {},
   "source": [
    "## Distil USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "failing-jacket",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_transformer = SentenceTransformer(\"distiluse-base-multilingual-cased-v2\")\n",
    "\n",
    "X_distiluse = []\n",
    "for name, X in tqdm(zip(names, X_datasets), total=4):\n",
    "    X_distiluse.append(\n",
    "        [\n",
    "            s_transformer.encode(sent_tokenize(x)).mean(axis=0)\n",
    "            for x in tqdm(X, desc=name)\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-reform",
   "metadata": {},
   "source": [
    "## Average GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "periodic-short",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_transformer = SentenceTransformer(\"average_word_embeddings_glove.6B.300d\")\n",
    "\n",
    "X_avg_glove = []\n",
    "for name, X in tqdm(zip(names, X_datasets), total=4):\n",
    "    X_avg_glove.append(\n",
    "        [\n",
    "            s_transformer.encode(sent_tokenize(x)).mean(axis=0)\n",
    "            for x in tqdm(X, desc=name)\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "needed-astrology",
   "metadata": {},
   "source": [
    "## STSB Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-convergence",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_transformer = SentenceTransformer(\"stsb-roberta-base\")\n",
    "\n",
    "X_roberta = []\n",
    "for name, X in tqdm(zip(names, X_datasets), total=4):\n",
    "    X_roberta.append(\n",
    "        [\n",
    "            s_transformer.encode(sent_tokenize(x)).mean(axis=0)\n",
    "            for x in tqdm(X, desc=name)\n",
    "        ]\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PA164 ML and NLP",
   "language": "python",
   "name": "pa164"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
